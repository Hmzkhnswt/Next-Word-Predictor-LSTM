{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"HAMZA ALI\n",
    "+92-330-3053300 • hamzakhanswati117191@gmail.com\n",
    "• Portfolio • GitHub • Kaggle • LinkedIn\n",
    " • Age: 22\n",
    "\n",
    "Objective\t\n",
    "As a student of Computer Systems Engineering at UET Peshawar, my objective is to leverage my skills and expertise in Data Science and Machine Learning by working as an internee at your company. I am seeking a role that will challenge me to continually grow and develop my skills while contributing to the success of the company.\n",
    "\n",
    "Education\n",
    "UNIVERSITY OF ENGINEERING AND TECHNOLOGY \tPeshawar, KPK\n",
    "Bachelor of Computer Systems Engineering\t2020-2024\n",
    "IIUI SCHOOLS AND COLLEGES\tMansehra, KPK\n",
    "Faculty of Science (Pre-Engineering)\t2018-2020\n",
    "Professional Experience\t\n",
    "AMSUS Technologies\tLahore, Pakistan\n",
    "WordPress Developer (Remote)\n",
    "October 2021 — January 2022\n",
    "\tTheme Customization: I have the skill for tailoring WordPress themes according to the site type\n",
    "\tPlugin Management: I learned how to add and configure useful plugins to enhance website's features\n",
    "\tResponsive Design: I got experience in creating websites that adapt to different screen sizes (e.g., mobile, tablet, desktop)\n",
    "TECHNOCOLABS \tIndore, India\n",
    "Data Science Intern (Remote) \n",
    "December 2022 – February 2023 \n",
    "\tData Analysis: Basic statistics for analyzing data also perform data wrangling and feature engineering for better model performance\n",
    "\tData Visualization: Basic insights of data to understand the data \n",
    "\tMachine Learning Operations: Training and testing of ML models and then deployment of the model using Flask\n",
    "Academic Experience\t\n",
    "Flask App for Age and Gender Prediction using CNN\n",
    "Age and Gender Prediction by using Functional Model\n",
    "VGG16: Use VGG16 as a convolutional base\n",
    "UTKFace: Kaggle dataset mostly use for training models for age and gender prediction \n",
    "Accuracy: Got accuracy of 91% with just 10 epochs\n",
    "App for Diabetes Type Prediction with Database for Patients Record\t\n",
    "Diabetes type prediction with database and deployment \n",
    "Decision Trees and SVM: Decision Trees as ML models with the accuracy of 97%\n",
    "Deployment of ML Model: Flask for the deployment of the model with register, login and dashboard \n",
    "My SQL Database: Database to store patient’s history\n",
    "Hand Written Digit Classification using ANN                                                                                    \t\n",
    "Training of Neural Networks to recognize the hand written digits having accuracy of 97% \n",
    "\tMNIST Dataset: I use MNIST dataset from KERAS having 70000 hand written words\n",
    "\tNeural Network Architecture: ANN having 101770 trainable parameters and SoftMax as an activation function\n",
    "Honors and Awards\t\n",
    "ONLINE COURSES \t\n",
    "Python for data science, AI and Development \t\t\t\t(Coursera 2021)\n",
    "Databases and SQL\t\t\t\t\t\t\t\t(Coursera 2022)\n",
    "Data Analysis\t\t\t\t\t\t\t\t(Coursera 2022)\n",
    "Data Visualization\t\t\t\t\t\t\t\t(Coursera 2022)\n",
    "Data Cleaning \t\t\t\t\t\t\t\t(Kaggle2022)\n",
    "Feature Engineering\t\t\t\t\t\t\t\t(Kaggle 2023)\n",
    "Data Manipulation with Pandas\t\t\t\t\t\t(Data Camp 2023)\n",
    "Machine Learning for Data Science \t\t\t\t\t\t(Coursera 2023)\n",
    "Deep Learning \t\t\t\t\t\t\t\t(CampusX 2023)\n",
    "Natural Language Processing\t\t\t\t\t\t(CampusX 2023)\n",
    "Additional\t\n",
    "Technical Skills: C++, Python, SQL, HTML, CSS, LINUX\n",
    "Interests: Sports, Fashion\n",
    "Language: Urdu, English, Hindko\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'and',\n",
       " 2: 'data',\n",
       " 3: 'of',\n",
       " 4: 'for',\n",
       " 5: 'to',\n",
       " 6: 'the',\n",
       " 7: '•',\n",
       " 8: 'engineering',\n",
       " 9: '2023',\n",
       " 10: 'with',\n",
       " 11: 'as',\n",
       " 12: 'science',\n",
       " 13: 'i',\n",
       " 14: '2022',\n",
       " 15: 'model',\n",
       " 16: 'prediction',\n",
       " 17: 'coursera',\n",
       " 18: 'age',\n",
       " 19: 'my',\n",
       " 20: 'learning',\n",
       " 21: 'deployment',\n",
       " 22: 'using',\n",
       " 23: 'accuracy',\n",
       " 24: 'database',\n",
       " 25: 'kaggle',\n",
       " 26: 'a',\n",
       " 27: 'skills',\n",
       " 28: 'machine',\n",
       " 29: 'experience',\n",
       " 30: 'type',\n",
       " 31: 'training',\n",
       " 32: 'ml',\n",
       " 33: 'models',\n",
       " 34: 'flask',\n",
       " 35: 'gender',\n",
       " 36: 'use',\n",
       " 37: 'dataset',\n",
       " 38: 'sql',\n",
       " 39: 'hand',\n",
       " 40: 'written',\n",
       " 41: 'having',\n",
       " 42: 'objective',\n",
       " 43: 'computer',\n",
       " 44: 'systems',\n",
       " 45: 'at',\n",
       " 46: 'peshawar',\n",
       " 47: 'in',\n",
       " 48: 'by',\n",
       " 49: 'an',\n",
       " 50: 'company',\n",
       " 51: 'that',\n",
       " 52: 'kpk',\n",
       " 53: '2020',\n",
       " 54: 'wordpress',\n",
       " 55: 'remote',\n",
       " 56: '2021',\n",
       " 57: 'got',\n",
       " 58: 'analysis',\n",
       " 59: 'basic',\n",
       " 60: 'feature',\n",
       " 61: 'visualization',\n",
       " 62: 'app',\n",
       " 63: 'vgg16',\n",
       " 64: 'diabetes',\n",
       " 65: 'decision',\n",
       " 66: 'trees',\n",
       " 67: '97',\n",
       " 68: 'ann',\n",
       " 69: 'neural',\n",
       " 70: 'mnist',\n",
       " 71: 'python',\n",
       " 72: 'campusx',\n",
       " 73: 'language',\n",
       " 74: 'hamza',\n",
       " 75: 'ali',\n",
       " 76: '92',\n",
       " 77: '330',\n",
       " 78: '3053300',\n",
       " 79: 'hamzakhanswati117191',\n",
       " 80: 'gmail',\n",
       " 81: 'com',\n",
       " 82: 'portfolio',\n",
       " 83: 'github',\n",
       " 84: 'linkedin',\n",
       " 85: '22',\n",
       " 86: 'student',\n",
       " 87: 'uet',\n",
       " 88: 'is',\n",
       " 89: 'leverage',\n",
       " 90: 'expertise',\n",
       " 91: 'working',\n",
       " 92: 'internee',\n",
       " 93: 'your',\n",
       " 94: 'am',\n",
       " 95: 'seeking',\n",
       " 96: 'role',\n",
       " 97: 'will',\n",
       " 98: 'challenge',\n",
       " 99: 'me',\n",
       " 100: 'continually',\n",
       " 101: 'grow',\n",
       " 102: 'develop',\n",
       " 103: 'while',\n",
       " 104: 'contributing',\n",
       " 105: 'success',\n",
       " 106: 'education',\n",
       " 107: 'university',\n",
       " 108: 'technology',\n",
       " 109: 'bachelor',\n",
       " 110: '2024',\n",
       " 111: 'iiui',\n",
       " 112: 'schools',\n",
       " 113: 'colleges',\n",
       " 114: 'mansehra',\n",
       " 115: 'faculty',\n",
       " 116: 'pre',\n",
       " 117: '2018',\n",
       " 118: 'professional',\n",
       " 119: 'amsus',\n",
       " 120: 'technologies',\n",
       " 121: 'lahore',\n",
       " 122: 'pakistan',\n",
       " 123: 'developer',\n",
       " 124: 'october',\n",
       " 125: '—',\n",
       " 126: 'january',\n",
       " 127: 'theme',\n",
       " 128: 'customization',\n",
       " 129: 'have',\n",
       " 130: 'skill',\n",
       " 131: 'tailoring',\n",
       " 132: 'themes',\n",
       " 133: 'according',\n",
       " 134: 'site',\n",
       " 135: 'plugin',\n",
       " 136: 'management',\n",
       " 137: 'learned',\n",
       " 138: 'how',\n",
       " 139: 'add',\n",
       " 140: 'configure',\n",
       " 141: 'useful',\n",
       " 142: 'plugins',\n",
       " 143: 'enhance',\n",
       " 144: \"website's\",\n",
       " 145: 'features',\n",
       " 146: 'responsive',\n",
       " 147: 'design',\n",
       " 148: 'creating',\n",
       " 149: 'websites',\n",
       " 150: 'adapt',\n",
       " 151: 'different',\n",
       " 152: 'screen',\n",
       " 153: 'sizes',\n",
       " 154: 'e',\n",
       " 155: 'g',\n",
       " 156: 'mobile',\n",
       " 157: 'tablet',\n",
       " 158: 'desktop',\n",
       " 159: 'technocolabs',\n",
       " 160: 'indore',\n",
       " 161: 'india',\n",
       " 162: 'intern',\n",
       " 163: 'december',\n",
       " 164: '–',\n",
       " 165: 'february',\n",
       " 166: 'statistics',\n",
       " 167: 'analyzing',\n",
       " 168: 'also',\n",
       " 169: 'perform',\n",
       " 170: 'wrangling',\n",
       " 171: 'better',\n",
       " 172: 'performance',\n",
       " 173: 'insights',\n",
       " 174: 'understand',\n",
       " 175: 'operations',\n",
       " 176: 'testing',\n",
       " 177: 'then',\n",
       " 178: 'academic',\n",
       " 179: 'cnn',\n",
       " 180: 'functional',\n",
       " 181: 'convolutional',\n",
       " 182: 'base',\n",
       " 183: 'utkface',\n",
       " 184: 'mostly',\n",
       " 185: '91',\n",
       " 186: 'just',\n",
       " 187: '10',\n",
       " 188: 'epochs',\n",
       " 189: 'patients',\n",
       " 190: 'record',\n",
       " 191: 'svm',\n",
       " 192: 'register',\n",
       " 193: 'login',\n",
       " 194: 'dashboard',\n",
       " 195: 'store',\n",
       " 196: 'patient’s',\n",
       " 197: 'history',\n",
       " 198: 'digit',\n",
       " 199: 'classification',\n",
       " 200: 'networks',\n",
       " 201: 'recognize',\n",
       " 202: 'digits',\n",
       " 203: 'from',\n",
       " 204: 'keras',\n",
       " 205: '70000',\n",
       " 206: 'words',\n",
       " 207: 'network',\n",
       " 208: 'architecture',\n",
       " 209: '101770',\n",
       " 210: 'trainable',\n",
       " 211: 'parameters',\n",
       " 212: 'softmax',\n",
       " 213: 'activation',\n",
       " 214: 'function',\n",
       " 215: 'honors',\n",
       " 216: 'awards',\n",
       " 217: 'online',\n",
       " 218: 'courses',\n",
       " 219: 'ai',\n",
       " 220: 'development',\n",
       " 221: 'databases',\n",
       " 222: 'cleaning',\n",
       " 223: 'kaggle2022',\n",
       " 224: 'manipulation',\n",
       " 225: 'pandas',\n",
       " 226: 'camp',\n",
       " 227: 'deep',\n",
       " 228: 'natural',\n",
       " 229: 'processing',\n",
       " 230: 'additional',\n",
       " 231: 'technical',\n",
       " 232: 'c',\n",
       " 233: 'html',\n",
       " 234: 'css',\n",
       " 235: 'linux',\n",
       " 236: 'interests',\n",
       " 237: 'sports',\n",
       " 238: 'fashion',\n",
       " 239: 'urdu',\n",
       " 240: 'english',\n",
       " 241: 'hindko'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.index_word)   ### so the vocabulary is 241 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74, 75]\n",
      "[76, 77, 78, 7, 79, 80, 81]\n",
      "[7, 82, 7, 83, 7, 25, 7, 84]\n",
      "[7, 18, 85]\n",
      "[]\n",
      "[42]\n",
      "[11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99, 5, 100, 101, 1, 102, 19, 27, 103, 104, 5, 6, 105, 3, 6, 50]\n",
      "[]\n",
      "[106]\n",
      "[107, 3, 8, 1, 108, 46, 52]\n",
      "[109, 3, 43, 44, 8, 53, 110]\n",
      "[111, 112, 1, 113, 114, 52]\n",
      "[115, 3, 12, 116, 8, 117, 53]\n",
      "[118, 29]\n",
      "[119, 120, 121, 122]\n",
      "[54, 123, 55]\n",
      "[124, 56, 125, 126, 14]\n",
      "[127, 128, 13, 129, 6, 130, 4, 131, 54, 132, 133, 5, 6, 134, 30]\n",
      "[135, 136, 13, 137, 138, 5, 139, 1, 140, 141, 142, 5, 143, 144, 145]\n",
      "[146, 147, 13, 57, 29, 47, 148, 149, 51, 150, 5, 151, 152, 153, 154, 155, 156, 157, 158]\n",
      "[159, 160, 161]\n",
      "[2, 12, 162, 55]\n",
      "[163, 14, 164, 165, 9]\n",
      "[2, 58, 59, 166, 4, 167, 2, 168, 169, 2, 170, 1, 60, 8, 4, 171, 15, 172]\n",
      "[2, 61, 59, 173, 3, 2, 5, 174, 6, 2]\n",
      "[28, 20, 175, 31, 1, 176, 3, 32, 33, 1, 177, 21, 3, 6, 15, 22, 34]\n",
      "[178, 29]\n",
      "[34, 62, 4, 18, 1, 35, 16, 22, 179]\n",
      "[18, 1, 35, 16, 48, 22, 180, 15]\n",
      "[63, 36, 63, 11, 26, 181, 182]\n",
      "[183, 25, 37, 184, 36, 4, 31, 33, 4, 18, 1, 35, 16]\n",
      "[23, 57, 23, 3, 185, 10, 186, 187, 188]\n",
      "[62, 4, 64, 30, 16, 10, 24, 4, 189, 190]\n",
      "[64, 30, 16, 10, 24, 1, 21]\n",
      "[65, 66, 1, 191, 65, 66, 11, 32, 33, 10, 6, 23, 3, 67]\n",
      "[21, 3, 32, 15, 34, 4, 6, 21, 3, 6, 15, 10, 192, 193, 1, 194]\n",
      "[19, 38, 24, 24, 5, 195, 196, 197]\n",
      "[39, 40, 198, 199, 22, 68]\n",
      "[31, 3, 69, 200, 5, 201, 6, 39, 40, 202, 41, 23, 3, 67]\n",
      "[70, 37, 13, 36, 70, 37, 203, 204, 41, 205, 39, 40, 206]\n",
      "[69, 207, 208, 68, 41, 209, 210, 211, 1, 212, 11, 49, 213, 214]\n",
      "[215, 1, 216]\n",
      "[217, 218]\n",
      "[71, 4, 2, 12, 219, 1, 220, 17, 56]\n",
      "[221, 1, 38, 17, 14]\n",
      "[2, 58, 17, 14]\n",
      "[2, 61, 17, 14]\n",
      "[2, 222, 223]\n",
      "[60, 8, 25, 9]\n",
      "[2, 224, 10, 225, 2, 226, 9]\n",
      "[28, 20, 4, 2, 12, 17, 9]\n",
      "[227, 20, 72, 9]\n",
      "[228, 73, 229, 72, 9]\n",
      "[230]\n",
      "[231, 27, 232, 71, 38, 233, 234, 235]\n",
      "[236, 237, 238]\n",
      "[73, 239, 240, 241]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for sentences in text.split('\\n'):\n",
    "    print(tokenizer.texts_to_sequences([sentences])[0])            ### tokens of sentence words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "\n",
    "for sentences in text.split('\\n'):\n",
    "    tokenized_sentence = tokenizer.texts_to_sequences([sentences])[0]      ### tokens of sentence words \n",
    "    \n",
    "    for i in range(1,len(tokenized_sentence)):\n",
    "        input_sequences.append(tokenized_sentence[:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[74, 75], [76, 77], [76, 77, 78], [76, 77, 78, 7], [76, 77, 78, 7, 79], [76, 77, 78, 7, 79, 80], [76, 77, 78, 7, 79, 80, 81], [7, 82], [7, 82, 7], [7, 82, 7, 83], [7, 82, 7, 83, 7], [7, 82, 7, 83, 7, 25], [7, 82, 7, 83, 7, 25, 7], [7, 82, 7, 83, 7, 25, 7, 84], [7, 18], [7, 18, 85], [11, 26], [11, 26, 86], [11, 26, 86, 3], [11, 26, 86, 3, 43], [11, 26, 86, 3, 43, 44], [11, 26, 86, 3, 43, 44, 8], [11, 26, 86, 3, 43, 44, 8, 45], [11, 26, 86, 3, 43, 44, 8, 45, 87], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99, 5], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99, 5, 100], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99, 5, 100, 101], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99, 5, 100, 101, 1], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99, 5, 100, 101, 1, 102], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99, 5, 100, 101, 1, 102, 19], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99, 5, 100, 101, 1, 102, 19, 27], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99, 5, 100, 101, 1, 102, 19, 27, 103], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99, 5, 100, 101, 1, 102, 19, 27, 103, 104], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99, 5, 100, 101, 1, 102, 19, 27, 103, 104, 5], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99, 5, 100, 101, 1, 102, 19, 27, 103, 104, 5, 6], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99, 5, 100, 101, 1, 102, 19, 27, 103, 104, 5, 6, 105], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99, 5, 100, 101, 1, 102, 19, 27, 103, 104, 5, 6, 105, 3], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99, 5, 100, 101, 1, 102, 19, 27, 103, 104, 5, 6, 105, 3, 6], [11, 26, 86, 3, 43, 44, 8, 45, 87, 46, 19, 42, 88, 5, 89, 19, 27, 1, 90, 47, 2, 12, 1, 28, 20, 48, 91, 11, 49, 92, 45, 93, 50, 13, 94, 95, 26, 96, 51, 97, 98, 99, 5, 100, 101, 1, 102, 19, 27, 103, 104, 5, 6, 105, 3, 6, 50], [107, 3], [107, 3, 8], [107, 3, 8, 1], [107, 3, 8, 1, 108], [107, 3, 8, 1, 108, 46], [107, 3, 8, 1, 108, 46, 52], [109, 3], [109, 3, 43], [109, 3, 43, 44], [109, 3, 43, 44, 8], [109, 3, 43, 44, 8, 53], [109, 3, 43, 44, 8, 53, 110], [111, 112], [111, 112, 1], [111, 112, 1, 113], [111, 112, 1, 113, 114], [111, 112, 1, 113, 114, 52], [115, 3], [115, 3, 12], [115, 3, 12, 116], [115, 3, 12, 116, 8], [115, 3, 12, 116, 8, 117], [115, 3, 12, 116, 8, 117, 53], [118, 29], [119, 120], [119, 120, 121], [119, 120, 121, 122], [54, 123], [54, 123, 55], [124, 56], [124, 56, 125], [124, 56, 125, 126], [124, 56, 125, 126, 14], [127, 128], [127, 128, 13], [127, 128, 13, 129], [127, 128, 13, 129, 6], [127, 128, 13, 129, 6, 130], [127, 128, 13, 129, 6, 130, 4], [127, 128, 13, 129, 6, 130, 4, 131], [127, 128, 13, 129, 6, 130, 4, 131, 54], [127, 128, 13, 129, 6, 130, 4, 131, 54, 132], [127, 128, 13, 129, 6, 130, 4, 131, 54, 132, 133], [127, 128, 13, 129, 6, 130, 4, 131, 54, 132, 133, 5], [127, 128, 13, 129, 6, 130, 4, 131, 54, 132, 133, 5, 6], [127, 128, 13, 129, 6, 130, 4, 131, 54, 132, 133, 5, 6, 134], [127, 128, 13, 129, 6, 130, 4, 131, 54, 132, 133, 5, 6, 134, 30], [135, 136], [135, 136, 13], [135, 136, 13, 137], [135, 136, 13, 137, 138], [135, 136, 13, 137, 138, 5], [135, 136, 13, 137, 138, 5, 139], [135, 136, 13, 137, 138, 5, 139, 1], [135, 136, 13, 137, 138, 5, 139, 1, 140], [135, 136, 13, 137, 138, 5, 139, 1, 140, 141], [135, 136, 13, 137, 138, 5, 139, 1, 140, 141, 142], [135, 136, 13, 137, 138, 5, 139, 1, 140, 141, 142, 5], [135, 136, 13, 137, 138, 5, 139, 1, 140, 141, 142, 5, 143], [135, 136, 13, 137, 138, 5, 139, 1, 140, 141, 142, 5, 143, 144], [135, 136, 13, 137, 138, 5, 139, 1, 140, 141, 142, 5, 143, 144, 145], [146, 147], [146, 147, 13], [146, 147, 13, 57], [146, 147, 13, 57, 29], [146, 147, 13, 57, 29, 47], [146, 147, 13, 57, 29, 47, 148], [146, 147, 13, 57, 29, 47, 148, 149], [146, 147, 13, 57, 29, 47, 148, 149, 51], [146, 147, 13, 57, 29, 47, 148, 149, 51, 150], [146, 147, 13, 57, 29, 47, 148, 149, 51, 150, 5], [146, 147, 13, 57, 29, 47, 148, 149, 51, 150, 5, 151], [146, 147, 13, 57, 29, 47, 148, 149, 51, 150, 5, 151, 152], [146, 147, 13, 57, 29, 47, 148, 149, 51, 150, 5, 151, 152, 153], [146, 147, 13, 57, 29, 47, 148, 149, 51, 150, 5, 151, 152, 153, 154], [146, 147, 13, 57, 29, 47, 148, 149, 51, 150, 5, 151, 152, 153, 154, 155], [146, 147, 13, 57, 29, 47, 148, 149, 51, 150, 5, 151, 152, 153, 154, 155, 156], [146, 147, 13, 57, 29, 47, 148, 149, 51, 150, 5, 151, 152, 153, 154, 155, 156, 157], [146, 147, 13, 57, 29, 47, 148, 149, 51, 150, 5, 151, 152, 153, 154, 155, 156, 157, 158], [159, 160], [159, 160, 161], [2, 12], [2, 12, 162], [2, 12, 162, 55], [163, 14], [163, 14, 164], [163, 14, 164, 165], [163, 14, 164, 165, 9], [2, 58], [2, 58, 59], [2, 58, 59, 166], [2, 58, 59, 166, 4], [2, 58, 59, 166, 4, 167], [2, 58, 59, 166, 4, 167, 2], [2, 58, 59, 166, 4, 167, 2, 168], [2, 58, 59, 166, 4, 167, 2, 168, 169], [2, 58, 59, 166, 4, 167, 2, 168, 169, 2], [2, 58, 59, 166, 4, 167, 2, 168, 169, 2, 170], [2, 58, 59, 166, 4, 167, 2, 168, 169, 2, 170, 1], [2, 58, 59, 166, 4, 167, 2, 168, 169, 2, 170, 1, 60], [2, 58, 59, 166, 4, 167, 2, 168, 169, 2, 170, 1, 60, 8], [2, 58, 59, 166, 4, 167, 2, 168, 169, 2, 170, 1, 60, 8, 4], [2, 58, 59, 166, 4, 167, 2, 168, 169, 2, 170, 1, 60, 8, 4, 171], [2, 58, 59, 166, 4, 167, 2, 168, 169, 2, 170, 1, 60, 8, 4, 171, 15], [2, 58, 59, 166, 4, 167, 2, 168, 169, 2, 170, 1, 60, 8, 4, 171, 15, 172], [2, 61], [2, 61, 59], [2, 61, 59, 173], [2, 61, 59, 173, 3], [2, 61, 59, 173, 3, 2], [2, 61, 59, 173, 3, 2, 5], [2, 61, 59, 173, 3, 2, 5, 174], [2, 61, 59, 173, 3, 2, 5, 174, 6], [2, 61, 59, 173, 3, 2, 5, 174, 6, 2], [28, 20], [28, 20, 175], [28, 20, 175, 31], [28, 20, 175, 31, 1], [28, 20, 175, 31, 1, 176], [28, 20, 175, 31, 1, 176, 3], [28, 20, 175, 31, 1, 176, 3, 32], [28, 20, 175, 31, 1, 176, 3, 32, 33], [28, 20, 175, 31, 1, 176, 3, 32, 33, 1], [28, 20, 175, 31, 1, 176, 3, 32, 33, 1, 177], [28, 20, 175, 31, 1, 176, 3, 32, 33, 1, 177, 21], [28, 20, 175, 31, 1, 176, 3, 32, 33, 1, 177, 21, 3], [28, 20, 175, 31, 1, 176, 3, 32, 33, 1, 177, 21, 3, 6], [28, 20, 175, 31, 1, 176, 3, 32, 33, 1, 177, 21, 3, 6, 15], [28, 20, 175, 31, 1, 176, 3, 32, 33, 1, 177, 21, 3, 6, 15, 22], [28, 20, 175, 31, 1, 176, 3, 32, 33, 1, 177, 21, 3, 6, 15, 22, 34], [178, 29], [34, 62], [34, 62, 4], [34, 62, 4, 18], [34, 62, 4, 18, 1], [34, 62, 4, 18, 1, 35], [34, 62, 4, 18, 1, 35, 16], [34, 62, 4, 18, 1, 35, 16, 22], [34, 62, 4, 18, 1, 35, 16, 22, 179], [18, 1], [18, 1, 35], [18, 1, 35, 16], [18, 1, 35, 16, 48], [18, 1, 35, 16, 48, 22], [18, 1, 35, 16, 48, 22, 180], [18, 1, 35, 16, 48, 22, 180, 15], [63, 36], [63, 36, 63], [63, 36, 63, 11], [63, 36, 63, 11, 26], [63, 36, 63, 11, 26, 181], [63, 36, 63, 11, 26, 181, 182], [183, 25], [183, 25, 37], [183, 25, 37, 184], [183, 25, 37, 184, 36], [183, 25, 37, 184, 36, 4], [183, 25, 37, 184, 36, 4, 31], [183, 25, 37, 184, 36, 4, 31, 33], [183, 25, 37, 184, 36, 4, 31, 33, 4], [183, 25, 37, 184, 36, 4, 31, 33, 4, 18], [183, 25, 37, 184, 36, 4, 31, 33, 4, 18, 1], [183, 25, 37, 184, 36, 4, 31, 33, 4, 18, 1, 35], [183, 25, 37, 184, 36, 4, 31, 33, 4, 18, 1, 35, 16], [23, 57], [23, 57, 23], [23, 57, 23, 3], [23, 57, 23, 3, 185], [23, 57, 23, 3, 185, 10], [23, 57, 23, 3, 185, 10, 186], [23, 57, 23, 3, 185, 10, 186, 187], [23, 57, 23, 3, 185, 10, 186, 187, 188], [62, 4], [62, 4, 64], [62, 4, 64, 30], [62, 4, 64, 30, 16], [62, 4, 64, 30, 16, 10], [62, 4, 64, 30, 16, 10, 24], [62, 4, 64, 30, 16, 10, 24, 4], [62, 4, 64, 30, 16, 10, 24, 4, 189], [62, 4, 64, 30, 16, 10, 24, 4, 189, 190], [64, 30], [64, 30, 16], [64, 30, 16, 10], [64, 30, 16, 10, 24], [64, 30, 16, 10, 24, 1], [64, 30, 16, 10, 24, 1, 21], [65, 66], [65, 66, 1], [65, 66, 1, 191], [65, 66, 1, 191, 65], [65, 66, 1, 191, 65, 66], [65, 66, 1, 191, 65, 66, 11], [65, 66, 1, 191, 65, 66, 11, 32], [65, 66, 1, 191, 65, 66, 11, 32, 33], [65, 66, 1, 191, 65, 66, 11, 32, 33, 10], [65, 66, 1, 191, 65, 66, 11, 32, 33, 10, 6], [65, 66, 1, 191, 65, 66, 11, 32, 33, 10, 6, 23], [65, 66, 1, 191, 65, 66, 11, 32, 33, 10, 6, 23, 3], [65, 66, 1, 191, 65, 66, 11, 32, 33, 10, 6, 23, 3, 67], [21, 3], [21, 3, 32], [21, 3, 32, 15], [21, 3, 32, 15, 34], [21, 3, 32, 15, 34, 4], [21, 3, 32, 15, 34, 4, 6], [21, 3, 32, 15, 34, 4, 6, 21], [21, 3, 32, 15, 34, 4, 6, 21, 3], [21, 3, 32, 15, 34, 4, 6, 21, 3, 6], [21, 3, 32, 15, 34, 4, 6, 21, 3, 6, 15], [21, 3, 32, 15, 34, 4, 6, 21, 3, 6, 15, 10], [21, 3, 32, 15, 34, 4, 6, 21, 3, 6, 15, 10, 192], [21, 3, 32, 15, 34, 4, 6, 21, 3, 6, 15, 10, 192, 193], [21, 3, 32, 15, 34, 4, 6, 21, 3, 6, 15, 10, 192, 193, 1], [21, 3, 32, 15, 34, 4, 6, 21, 3, 6, 15, 10, 192, 193, 1, 194], [19, 38], [19, 38, 24], [19, 38, 24, 24], [19, 38, 24, 24, 5], [19, 38, 24, 24, 5, 195], [19, 38, 24, 24, 5, 195, 196], [19, 38, 24, 24, 5, 195, 196, 197], [39, 40], [39, 40, 198], [39, 40, 198, 199], [39, 40, 198, 199, 22], [39, 40, 198, 199, 22, 68], [31, 3], [31, 3, 69], [31, 3, 69, 200], [31, 3, 69, 200, 5], [31, 3, 69, 200, 5, 201], [31, 3, 69, 200, 5, 201, 6], [31, 3, 69, 200, 5, 201, 6, 39], [31, 3, 69, 200, 5, 201, 6, 39, 40], [31, 3, 69, 200, 5, 201, 6, 39, 40, 202], [31, 3, 69, 200, 5, 201, 6, 39, 40, 202, 41], [31, 3, 69, 200, 5, 201, 6, 39, 40, 202, 41, 23], [31, 3, 69, 200, 5, 201, 6, 39, 40, 202, 41, 23, 3], [31, 3, 69, 200, 5, 201, 6, 39, 40, 202, 41, 23, 3, 67], [70, 37], [70, 37, 13], [70, 37, 13, 36], [70, 37, 13, 36, 70], [70, 37, 13, 36, 70, 37], [70, 37, 13, 36, 70, 37, 203], [70, 37, 13, 36, 70, 37, 203, 204], [70, 37, 13, 36, 70, 37, 203, 204, 41], [70, 37, 13, 36, 70, 37, 203, 204, 41, 205], [70, 37, 13, 36, 70, 37, 203, 204, 41, 205, 39], [70, 37, 13, 36, 70, 37, 203, 204, 41, 205, 39, 40], [70, 37, 13, 36, 70, 37, 203, 204, 41, 205, 39, 40, 206], [69, 207], [69, 207, 208], [69, 207, 208, 68], [69, 207, 208, 68, 41], [69, 207, 208, 68, 41, 209], [69, 207, 208, 68, 41, 209, 210], [69, 207, 208, 68, 41, 209, 210, 211], [69, 207, 208, 68, 41, 209, 210, 211, 1], [69, 207, 208, 68, 41, 209, 210, 211, 1, 212], [69, 207, 208, 68, 41, 209, 210, 211, 1, 212, 11], [69, 207, 208, 68, 41, 209, 210, 211, 1, 212, 11, 49], [69, 207, 208, 68, 41, 209, 210, 211, 1, 212, 11, 49, 213], [69, 207, 208, 68, 41, 209, 210, 211, 1, 212, 11, 49, 213, 214], [215, 1], [215, 1, 216], [217, 218], [71, 4], [71, 4, 2], [71, 4, 2, 12], [71, 4, 2, 12, 219], [71, 4, 2, 12, 219, 1], [71, 4, 2, 12, 219, 1, 220], [71, 4, 2, 12, 219, 1, 220, 17], [71, 4, 2, 12, 219, 1, 220, 17, 56], [221, 1], [221, 1, 38], [221, 1, 38, 17], [221, 1, 38, 17, 14], [2, 58], [2, 58, 17], [2, 58, 17, 14], [2, 61], [2, 61, 17], [2, 61, 17, 14], [2, 222], [2, 222, 223], [60, 8], [60, 8, 25], [60, 8, 25, 9], [2, 224], [2, 224, 10], [2, 224, 10, 225], [2, 224, 10, 225, 2], [2, 224, 10, 225, 2, 226], [2, 224, 10, 225, 2, 226, 9], [28, 20], [28, 20, 4], [28, 20, 4, 2], [28, 20, 4, 2, 12], [28, 20, 4, 2, 12, 17], [28, 20, 4, 2, 12, 17, 9], [227, 20], [227, 20, 72], [227, 20, 72, 9], [228, 73], [228, 73, 229], [228, 73, 229, 72], [228, 73, 229, 72, 9], [231, 27], [231, 27, 232], [231, 27, 232, 71], [231, 27, 232, 71, 38], [231, 27, 232, 71, 38, 233], [231, 27, 232, 71, 38, 233, 234], [231, 27, 232, 71, 38, 233, 234, 235], [236, 237], [236, 237, 238], [73, 239], [73, 239, 240], [73, 239, 240, 241]]\n"
     ]
    }
   ],
   "source": [
    "print(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394\n"
     ]
    }
   ],
   "source": [
    "print(len(input_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "max_len_sent = max([len(x) for x in input_sequences])          ### we do this so we can add zero padding for the input feild to equal the size of inputs\n",
    "print(max_len_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "padded_input_sequence = pad_sequences(input_sequences, max_len_sent+1,padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_input_sequence[:, :-1] \n",
    "y = padded_input_sequence[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(394, 57)\n",
      "(394,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now we will do the one hot encodeing of y means the output beacause we want one hot encoded output like [0 0 1 0] means the third word has the highest probabiliy\n",
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y, 242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394, 242)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(242, 100, input_length=57))     ### as we have added a zero embeddings so we will use the embedding to make denser from sparse\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(242, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 57, 100)           24200     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 150)               150600    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 242)               36542     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 211342 (825.55 KB)\n",
      "Trainable params: 211342 (825.55 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 3s 65ms/step - loss: 5.4810 - accuracy: 0.0406\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 1s 64ms/step - loss: 5.2994 - accuracy: 0.0482\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 1s 67ms/step - loss: 5.1315 - accuracy: 0.0482\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 1s 65ms/step - loss: 5.0781 - accuracy: 0.0355\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 5.0499 - accuracy: 0.0482\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 1s 68ms/step - loss: 5.0339 - accuracy: 0.0482\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 1s 64ms/step - loss: 5.0156 - accuracy: 0.0482\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 4.9893 - accuracy: 0.0482\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 4.9546 - accuracy: 0.0533\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 4.9027 - accuracy: 0.0736\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 4.8049 - accuracy: 0.0584\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 4.6966 - accuracy: 0.0609\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 1s 77ms/step - loss: 4.5536 - accuracy: 0.0888\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 4.3996 - accuracy: 0.0964\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 4.2458 - accuracy: 0.0964\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 4.0597 - accuracy: 0.1396\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 1s 77ms/step - loss: 3.8851 - accuracy: 0.1497\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 3.6959 - accuracy: 0.2107\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 3.4849 - accuracy: 0.2132\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 3.3033 - accuracy: 0.2589\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 3.1154 - accuracy: 0.2919\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 2.9403 - accuracy: 0.3223\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 2.7696 - accuracy: 0.3604\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 2.6274 - accuracy: 0.3959\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 2.4746 - accuracy: 0.4315\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 2.3322 - accuracy: 0.4670\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 2.1992 - accuracy: 0.5000\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 2.0747 - accuracy: 0.5381\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 1.9567 - accuracy: 0.5964\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 1s 78ms/step - loss: 1.8509 - accuracy: 0.6396\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 1.7547 - accuracy: 0.6624\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 1s 77ms/step - loss: 1.6574 - accuracy: 0.7157\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 1.5550 - accuracy: 0.7437\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 1s 78ms/step - loss: 1.4752 - accuracy: 0.7716\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 1s 78ms/step - loss: 1.3933 - accuracy: 0.7893\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 1.3238 - accuracy: 0.8198\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 1.2513 - accuracy: 0.8376\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 1.1765 - accuracy: 0.8680\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 1.1049 - accuracy: 0.8756\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 1s 78ms/step - loss: 1.0496 - accuracy: 0.8959\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 1.0008 - accuracy: 0.9036\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 0.9439 - accuracy: 0.9010\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 0.8947 - accuracy: 0.9162\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 0.8475 - accuracy: 0.9239\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 0.7993 - accuracy: 0.9365\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 0.7573 - accuracy: 0.9365\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 0.7183 - accuracy: 0.9492\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 1s 77ms/step - loss: 0.6802 - accuracy: 0.9518\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 0.6471 - accuracy: 0.9492\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.6148 - accuracy: 0.9543\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 0.5901 - accuracy: 0.9619\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 0.5579 - accuracy: 0.9569\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 0.5330 - accuracy: 0.9619\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 0.5067 - accuracy: 0.9670\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 0.4853 - accuracy: 0.9645\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 0.4662 - accuracy: 0.9645\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 0.4415 - accuracy: 0.9670\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.4215 - accuracy: 0.9594\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 0.4033 - accuracy: 0.9670\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 0.3870 - accuracy: 0.9645\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 0.3720 - accuracy: 0.9695\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 0.3577 - accuracy: 0.9645\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.3407 - accuracy: 0.9695\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 0.3279 - accuracy: 0.9695\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 0.3142 - accuracy: 0.9695\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 0.3050 - accuracy: 0.9721\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 0.2950 - accuracy: 0.9670\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 1s 78ms/step - loss: 0.2835 - accuracy: 0.9670\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 1s 78ms/step - loss: 0.2738 - accuracy: 0.9695\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 0.2627 - accuracy: 0.9670\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 0.2538 - accuracy: 0.9670\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 0.2460 - accuracy: 0.9721\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 0.2363 - accuracy: 0.9721\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 0.2302 - accuracy: 0.9670\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 0.2219 - accuracy: 0.9721\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 0.2151 - accuracy: 0.9772\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 0.2088 - accuracy: 0.9721\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 0.2032 - accuracy: 0.9721\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 0.1987 - accuracy: 0.9772\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 0.1928 - accuracy: 0.9772\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 0.1881 - accuracy: 0.9721\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 0.1821 - accuracy: 0.9746\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.1784 - accuracy: 0.9772\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 0.1730 - accuracy: 0.9797\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 0.1695 - accuracy: 0.9721\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 0.1667 - accuracy: 0.9695\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 0.1612 - accuracy: 0.9772\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 0.1582 - accuracy: 0.9746\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 0.1537 - accuracy: 0.9721\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 0.1505 - accuracy: 0.9721\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 1s 81ms/step - loss: 0.1481 - accuracy: 0.9772\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 0.1451 - accuracy: 0.9772\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 1s 83ms/step - loss: 0.1421 - accuracy: 0.9772\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 1s 84ms/step - loss: 0.1408 - accuracy: 0.9721\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 0.1371 - accuracy: 0.9721\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.1336 - accuracy: 0.9695\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 1s 87ms/step - loss: 0.1313 - accuracy: 0.9746\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 1s 82ms/step - loss: 0.1281 - accuracy: 0.9721\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 1s 85ms/step - loss: 0.1266 - accuracy: 0.9721\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 1s 86ms/step - loss: 0.1253 - accuracy: 0.9746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18584234520>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "Machine Learning for\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "Machine Learning for data\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Machine Learning for data science\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Machine Learning for data science coursera\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Machine Learning for data science coursera 2023\n"
     ]
    }
   ],
   "source": [
    "test_text = 'Machine Learning'\n",
    "\n",
    "for i in range(5):\n",
    "    ### tokenization of test text\n",
    "    token_test_text = tokenizer.texts_to_sequences([test_text])[0]\n",
    "    ### padding for the same length of the input\n",
    "    padded_test_text = pad_sequences([token_test_text], maxlen=57, padding='pre')\n",
    "    ### prediction \n",
    "    predicted_next_word = np.argmax(model.predict(padded_test_text))\n",
    "    ### output will be (1,242) is the shape\n",
    "\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if index == predicted_next_word:\n",
    "            test_text = test_text+ \" \"+ word\n",
    "            print(test_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
